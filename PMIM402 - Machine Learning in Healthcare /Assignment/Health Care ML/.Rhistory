nb_pred <- predict(nb_model, newdata = df_test, type = "raw")
View(nb_pred)
View(nb_pred)
table(predicted = nb_pred, observed = df_test$class)
#######
nb_pred <- predict(nb_model, newdata = df_test, type = "class")
table(predicted = nb_pred, observed = df_test$class)
ls(nb_model)
####### predicting and plotting CM
nb_pred <- predict(nb_model, newdata = df_test, type = "raw")
roc(nb_pred, df_test$class, plot=TRUE)
df$class <- as.factor(df$class)
bound <- floor((nrow(df)/4)*3)   # 75-25% training testing split
df <- df[sample(nrow(df)),]          # shuffles the data
df_train <- df[1:bound, ]              # get training set
df_test <- df[(bound+1):nrow(df), ]    # get test set
### Naive Bayes
nb_model <- naiveBayes(class ~ ., data = df_train)
####### predicting and plotting CM
nb_pred <- predict(nb_model, newdata = df_test, type = "raw")
roc(nb_pred, df_test$class, plot=TRUE)
print(nb_pred)
## Trying to print ROC curve
nb_pred %>%
roc_curve(truth=df_test$class , nb_pred) %>%
autoplot()
roc(nb_pred, df_test$class, plot=TRUE)
#### Random Forrest
rffit <- randomForest(df_train,df_train$class)
plot(rffit)
varImpPlot(rffit)
roc(rffit, df_test$class, plot=TRUE)
rf_pred <- predicted(rffit, newdata = df_test, type = "raw")
library(e1071) # all purpose machine learning package
options(scipen=999)
options(repr.plot.width = 15, repr.plot.height = 20)
library(tidyverse)
library(randomForest)
library(ROCR)
df<- read.csv("2. Classification/heart_disease_modified.csv")
df <- na.omit(df)
df <- within(df, rm(X, Patient_ID))
df$class <- as.factor(df$class)
bound <- floor((nrow(df)/4)*3)   # 75-25% training testing split
df <- df[sample(nrow(df)),]          # shuffles the data
df_train <- df[1:bound, ]              # get training set
df_test <- df[(bound+1):nrow(df), ]    # get test set
### Naive Bayes
nb_model <- naiveBayes(class ~ ., data = df_train)
#
##Output apriori and conditional probabilities
print(nb_model)
####### predicting and plotting CM
nb_pred <- predict(nb_model, newdata = df_test)
table(predicted = nb_pred, observed = df_test$class)
forest_train <- randomForest(class ~ ., data = df_train)
print(forest_train) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train)
testforest = predict(forest_train, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
test.forest = predict(forest_train, type = "prob", newdata = df_test)
forestpred = prediction(test.forest[,2], df_test$class)
forestperf = performance(forestpred, "tpr", "fpr")
plot(forestperf, main="ROC", colorize=T)#
#plot(bagperf, col=2, add=TRUE)#
#plot(perf, col=1, add=TRUE)#
plot(forestperf, col=3, add=TRUE)
test.nb = predict(nb_model, type = "raw", newdata = df_test)
nbpred = prediction(test.nb[,2], df_test$class)
nbperf = performance(nbpred, "tpr", "fpr")
plot(nbperf, main="ROC", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
#plot(perf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)
legend(0.6, 0.6, c('rforest', 'Naive Bayes'), 1:3)
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 4)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest = predict(forest_train2, newdata=df_test)
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC", colorize=T)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf, col=1, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 10)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest = predict(forest_train2, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC", colorize=T)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf, col=1, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 2)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest = predict(forest_train2, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC", colorize=T)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf, col=1, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 20)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest = predict(forest_train2, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC", colorize=T)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf, col=1, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 200)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest = predict(forest_train2, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC", colorize=T)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf, col=1, add=TRUE)#
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 500)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest = predict(forest_train2, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC", colorize=T)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf, col=1, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 700)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest = predict(forest_train2, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC", colorize=T)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf, col=1, add=TRUE)#
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 900)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest = predict(forest_train2, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC", colorize=T)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf, col=1, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
plot(forestperf2, main="ROC for 3 Models", colorize=T)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf, col=1, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
legend(0.6, 0.6, c('rforest', 'Naive Bayes'), 1:3)
plot(nbperf, main="Naive Bayes ROC", colorize=T)#
plot(nbperf, col=2, add=TRUE)
legend(0.6, 0.6, c('rforest', 'Naive Bayes'), 1:3)
legend(0.6, 0.6, c('', 'Naive Bayes'), 1:3)
legend(0.6, 0.6, c(Naive Bayes'), 1:3)
#### Random Forrest
install.packages("randomForest")
library(randomForest)
library(ROCR)
forest_train <- randomForest(class ~ ., data = df_train)
print(forest_train) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train)
testforest = predict(forest_train, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
test.forest = predict(forest_train, type = "prob", newdata = df_test)
forestpred = prediction(test.forest[,2], df_test$class)
forestperf = performance(forestpred, "tpr", "fpr")
plot(forestperf, main="ROC", colorize=T)#
plot(forestperf, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest'), 1:3)
varImpPlot(rffit)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 900)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest = predict(forest_train2, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
#control <- trainControl(method="cv", number=5, savePredictions = TRUE, classProbs = TRUE)
#control
#
#head(df_train)
#
#svmFit <- train(class ~ sex + age + cp + trestbps + chol + fbs + restecg + thalach, data = df_train,
#                method = "svmLinear",
#                trControl = control
#)
#
#str(df_train$class)
#
#
#### Experimenting
#library(ROCR)
## Calculate the probability of new observations belonging to each class
## prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
#prediction_for_roc_curve <- predict(rffit,df_test[,-5],type="prob")
## Use pretty colours:
#pretty_colours <- c("#F8766D","#00BA38","#619CFF")
## Specify the different classes
#classes <- levels(df_test$class)
## For each class
#for (i in 1:2)
#{
#  # Define which observations belong to class[i]
#  true_values <- ifelse(df_test[,5]==classes[i],1,0)
#  # Assess the performance of classifier for class[i]
#  pred <- prediction(prediction_for_roc_curve[,i],true_values)
#  perf <- performance(pred, "tpr", "fpr")
#  if (i==1)
#  {
#    plot(perf,main="ROC Curve",col=pretty_colours[i])
#  }
#  else
#  {
#    plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE)
#  }
#  # Calculate the AUC and print it to screen
#  auc.perf <- performance(pred, measure = "auc")
#  print(auc.perf@y.values)
#}
legend(0.6, 0.6, c('Naive Bayes'), 1:3)
test.nb = predict(nb_model, type = "raw", newdata = df_test)
nbpred = prediction(test.nb[,2], df_test$class)
nbperf = performance(nbpred, "tpr", "fpr")
plot(nbperf, main="Naive Bayes ROC", colorize=T)#
plot(nbperf, col=2, add=TRUE)
legend(0.6, 0.6, c('Naive Bayes'), 1:3)
plot(nbperf, col=1, add=TRUE)
legend(0.6, 0.6, c('Naive Bayes'), 1:3)
test.forest = predict(forest_train, type = "prob", newdata = df_test)
forestpred = prediction(test.forest[,2], df_test$class)
forestperf = performance(forestpred, "tpr", "fpr")
plot(forestperf, main="Random Forrest ROC", colorize=T)#
plot(forestperf, col=1, add=TRUE)
legend(0.6, 0.6, c('rforest'), 1:3)
table(testforest, df_test$class) #confusion matrix for test set
table(testforest, df_test$class) #confusion matrix for test set
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 900)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
testforest = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 400)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 550)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
print(forest_train) #notice the number of trees, number of splits and the confusion matrix
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 300)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 900)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 600)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 600, mtry = 6)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 480, mtry = 3)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
# Compare
print(forest_train)
print(forest_train2)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 480, mtry = 5)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
# Compare
print(forest_train)
print(forest_train2)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 500, mtry = 5)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
# Compare
print(forest_train)
print(forest_train2)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 500, mtry = 7)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
# Compare
print(forest_train)
print(forest_train2)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 550, mtry = 2)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
# Compare
print(forest_train)
print(forest_train2)
varImpPlot(rffit)
forest_train3 <- randomForest(class ~ cp + thal + thalach + exang + oldpeak + age + chol, data = df_train, ntree = 550, mtry = 2)
print(forest_train3) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train3)
testforest3 = predict(forest_train3, newdata=df_test)
table(testforest3, df_test$class) #confusion matrix for test set
test.forest3 = predict(forest_train3, type = "prob", newdata = df_test)
forestpred3 = prediction(test.forest3[,2], df_test$class)
forestperf3 = performance(forestpred3, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
plot(forestperf3, col=4, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt', 'rforest opt 2'), 1:3)
plot(forestperf3, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
plot(forestperf3, col=4, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt', 'rforest opt 2'), 1:3)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt', 'rforest opt 2'), 1:4)
forest_train3 <- randomForest(class ~ cp + thal + thalach + exang + oldpeak + age + chol, data = df_train)
print(forest_train3) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train3)
testforest3 = predict(forest_train3, newdata=df_test)
table(testforest3, df_test$class) #confusion matrix for test set
test.forest3 = predict(forest_train3, type = "prob", newdata = df_test)
forestpred3 = prediction(test.forest3[,2], df_test$class)
forestperf3 = performance(forestpred3, "tpr", "fpr")
plot(forestperf3, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
plot(forestperf3, col=4, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt', 'rforest opt 2'), 1:4)
# Compare
print(forest_train)
print(forest_train2)
print(forest_train3)
auc_ROCR <- performance(nbpred, measure = "auc")
auc_ROCR <- auc_ROCR@y.values[[1]]
print(auc_ROCR)
auc_nb <- performance(nbpred, measure = "auc")
auc_nb <- auc_nb@y.values[[1]]
print(auc_nb)
auc_nb <- performance(nbpred, measure = "auc")
auc_nb <- auc_nb@y.values[[1]]
auc_rf <- performance(forestpred, measure = "auc")
auc_rf <- forestpred@y.values[[1]]
auc_rf2 <- performance(forestpred2, measure = "auc")
auc_rf2 <- auc_rf2@y.values[[1]]
auc_rf <- performance(forestpred, measure = "auc")
auc_rf <- auc_rf@y.values[[1]]
auc_rf2 <- performance(forestpred2, measure = "auc")
auc_rf2 <- auc_rf2@y.values[[1]]
print(auc_nb)
print(auc_rf)
print(auc_rf2)
library(e1071) # all purpose machine learning package
library(caret)
confusionMatrix(nbpred, df_test$class)
confusionMatrix(nb_pred, df_test$class)
confusionMatrix(nb_pred, df_test$class)
confusionMatrix(testforest, df_test$class)
confusionMatrix(testforest2, df_test$class)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 800, mtry = 4, maxnodes=24)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
confusionMatrix(testforest, df_test$class)
confusionMatrix(testforest2, df_test$class)
##### AUC values
auc_nb <- performance(nbpred, measure = "auc")
auc_nb <- auc_nb@y.values[[1]]
auc_rf <- performance(forestpred, measure = "auc")
auc_rf <- auc_rf@y.values[[1]]
auc_rf2 <- performance(forestpred2, measure = "auc")
auc_rf2 <- auc_rf2@y.values[[1]]
print(auc_nb)
print(auc_rf)
print(auc_rf2)
confusionMatrix(nb_pred, df_test$class)
# Compare
print(forest_train)
print(forest_train2)
auc_rf2 <- performance(forestpred2, measure = "auc")
auc_rf2 <- auc_rf2@y.values[[1]]
print(auc_rf2)
print(auc_rf)
print(nb_pred)
