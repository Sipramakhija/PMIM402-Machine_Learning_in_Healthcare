forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 900)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest = predict(forest_train2, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
#control <- trainControl(method="cv", number=5, savePredictions = TRUE, classProbs = TRUE)
#control
#
#head(df_train)
#
#svmFit <- train(class ~ sex + age + cp + trestbps + chol + fbs + restecg + thalach, data = df_train,
#                method = "svmLinear",
#                trControl = control
#)
#
#str(df_train$class)
#
#
#### Experimenting
#library(ROCR)
## Calculate the probability of new observations belonging to each class
## prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
#prediction_for_roc_curve <- predict(rffit,df_test[,-5],type="prob")
## Use pretty colours:
#pretty_colours <- c("#F8766D","#00BA38","#619CFF")
## Specify the different classes
#classes <- levels(df_test$class)
## For each class
#for (i in 1:2)
#{
#  # Define which observations belong to class[i]
#  true_values <- ifelse(df_test[,5]==classes[i],1,0)
#  # Assess the performance of classifier for class[i]
#  pred <- prediction(prediction_for_roc_curve[,i],true_values)
#  perf <- performance(pred, "tpr", "fpr")
#  if (i==1)
#  {
#    plot(perf,main="ROC Curve",col=pretty_colours[i])
#  }
#  else
#  {
#    plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE)
#  }
#  # Calculate the AUC and print it to screen
#  auc.perf <- performance(pred, measure = "auc")
#  print(auc.perf@y.values)
#}
legend(0.6, 0.6, c('Naive Bayes'), 1:3)
test.nb = predict(nb_model, type = "raw", newdata = df_test)
nbpred = prediction(test.nb[,2], df_test$class)
nbperf = performance(nbpred, "tpr", "fpr")
plot(nbperf, main="Naive Bayes ROC", colorize=T)#
plot(nbperf, col=2, add=TRUE)
legend(0.6, 0.6, c('Naive Bayes'), 1:3)
plot(nbperf, col=1, add=TRUE)
legend(0.6, 0.6, c('Naive Bayes'), 1:3)
test.forest = predict(forest_train, type = "prob", newdata = df_test)
forestpred = prediction(test.forest[,2], df_test$class)
forestperf = performance(forestpred, "tpr", "fpr")
plot(forestperf, main="Random Forrest ROC", colorize=T)#
plot(forestperf, col=1, add=TRUE)
legend(0.6, 0.6, c('rforest'), 1:3)
table(testforest, df_test$class) #confusion matrix for test set
table(testforest, df_test$class) #confusion matrix for test set
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 900)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
testforest = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 400)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 550)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
print(forest_train) #notice the number of trees, number of splits and the confusion matrix
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 300)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 900)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 600)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 600, mtry = 6)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 480, mtry = 3)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
# Compare
print(forest_train)
print(forest_train2)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 480, mtry = 5)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
# Compare
print(forest_train)
print(forest_train2)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 500, mtry = 5)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
# Compare
print(forest_train)
print(forest_train2)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 500, mtry = 7)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
# Compare
print(forest_train)
print(forest_train2)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 550, mtry = 2)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
# Compare
print(forest_train)
print(forest_train2)
varImpPlot(rffit)
forest_train3 <- randomForest(class ~ cp + thal + thalach + exang + oldpeak + age + chol, data = df_train, ntree = 550, mtry = 2)
print(forest_train3) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train3)
testforest3 = predict(forest_train3, newdata=df_test)
table(testforest3, df_test$class) #confusion matrix for test set
test.forest3 = predict(forest_train3, type = "prob", newdata = df_test)
forestpred3 = prediction(test.forest3[,2], df_test$class)
forestperf3 = performance(forestpred3, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
plot(forestperf3, col=4, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt', 'rforest opt 2'), 1:3)
plot(forestperf3, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
plot(forestperf3, col=4, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt', 'rforest opt 2'), 1:3)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt', 'rforest opt 2'), 1:4)
forest_train3 <- randomForest(class ~ cp + thal + thalach + exang + oldpeak + age + chol, data = df_train)
print(forest_train3) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train3)
testforest3 = predict(forest_train3, newdata=df_test)
table(testforest3, df_test$class) #confusion matrix for test set
test.forest3 = predict(forest_train3, type = "prob", newdata = df_test)
forestpred3 = prediction(test.forest3[,2], df_test$class)
forestperf3 = performance(forestpred3, "tpr", "fpr")
plot(forestperf3, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
plot(forestperf3, col=4, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt', 'rforest opt 2'), 1:4)
# Compare
print(forest_train)
print(forest_train2)
print(forest_train3)
auc_ROCR <- performance(nbpred, measure = "auc")
auc_ROCR <- auc_ROCR@y.values[[1]]
print(auc_ROCR)
auc_nb <- performance(nbpred, measure = "auc")
auc_nb <- auc_nb@y.values[[1]]
print(auc_nb)
auc_nb <- performance(nbpred, measure = "auc")
auc_nb <- auc_nb@y.values[[1]]
auc_rf <- performance(forestpred, measure = "auc")
auc_rf <- forestpred@y.values[[1]]
auc_rf2 <- performance(forestpred2, measure = "auc")
auc_rf2 <- auc_rf2@y.values[[1]]
auc_rf <- performance(forestpred, measure = "auc")
auc_rf <- auc_rf@y.values[[1]]
auc_rf2 <- performance(forestpred2, measure = "auc")
auc_rf2 <- auc_rf2@y.values[[1]]
print(auc_nb)
print(auc_rf)
print(auc_rf2)
library(e1071) # all purpose machine learning package
library(caret)
confusionMatrix(nbpred, df_test$class)
confusionMatrix(nb_pred, df_test$class)
confusionMatrix(nb_pred, df_test$class)
confusionMatrix(testforest, df_test$class)
confusionMatrix(testforest2, df_test$class)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 800, mtry = 4, maxnodes=24)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
confusionMatrix(testforest, df_test$class)
confusionMatrix(testforest2, df_test$class)
##### AUC values
auc_nb <- performance(nbpred, measure = "auc")
auc_nb <- auc_nb@y.values[[1]]
auc_rf <- performance(forestpred, measure = "auc")
auc_rf <- auc_rf@y.values[[1]]
auc_rf2 <- performance(forestpred2, measure = "auc")
auc_rf2 <- auc_rf2@y.values[[1]]
print(auc_nb)
print(auc_rf)
print(auc_rf2)
confusionMatrix(nb_pred, df_test$class)
# Compare
print(forest_train)
print(forest_train2)
auc_rf2 <- performance(forestpred2, measure = "auc")
auc_rf2 <- auc_rf2@y.values[[1]]
print(auc_rf2)
print(auc_rf)
print(nb_pred)
#install.packages("e1071")
#install.packages('factoextra')
library(e1071) # all purpose machine learning package
library(reshape2) # library to reshape data
library(ggplot2)
options(scipen=999)
library(tidyverse) # data manipulation
library(cluster) # clustering algorithms
library(factoextra)
options(repr.plot.width = 15, repr.plot.height = 20)
########################### Load in dataset ####################################
df<- read.csv("1. Clustering/heart-c.csv")
df_og <- df
df <- na.omit(df)
for (i in 1:nrow(df['sex'])){
if (df['sex'][i,1] == 'male') {
df['sex'][i,1] = as.numeric(1)
}
else if (df['sex'][i,1] == 'female') {
df['sex'][i,1] = as.numeric(0)
}
}
print(df['sex'])
for (i in 1:nrow(df['cp'])){
if (df['cp'][i,1] == 'typ_angina') {
df['cp'][i,1] = as.numeric(1)
}
else if (df['cp'][i,1] == 'atyp_angina') {
df['cp'][i,1] = as.numeric(2)
}
else if (df['cp'][i,1] == 'non_anginal') {
df['cp'][i,1] = as.numeric(3)
}
else if (df['cp'][i,1] == 'asympt') {
df['cp'][i,1] = as.numeric(4)
}
}
for (i in 1:nrow(df['fbs'])){
if (df['fbs'][i,1] == 't') {
df['fbs'][i,1] = as.numeric(1)
}
else if (df['fbs'][i,1] == 'f') {
df['fbs'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['restecg'])){
if (df['restecg'][i,1] == 'normal') {
df['restecg'][i,1] = as.numeric(0)
}
else if (df['restecg'][i,1] == 'st_t_wave_abnormality') {
df['restecg'][i,1] = as.numeric(1)
}
else if (df['restecg'][i,1] == 'left_vent_hyper') {
df['restecg'][i,1] = as.numeric(2)
}
}
for (i in 1:nrow(df['exang'])){
if (df['exang'][i,1] == 'yes') {
df['exang'][i,1] = as.numeric(1)
}
else if (df['exang'][i,1] == 'no') {
df['exang'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['slope'])){
if (df['slope'][i,1] == 'up') {
df['slope'][i,1] = as.numeric(1)
}
else if (df['slope'][i,1] == 'flat') {
df['slope'][i,1] = as.numeric(2)
}
else if (df['slope'][i,1] == 'down') {
df['slope'][i,1] = as.numeric(3)
}
}
for (i in 1:nrow(df['thal'])){
if (df['thal'][i,1] == 'normal') {
df['thal'][i,1] = as.numeric(3)
}
else if (df['thal'][i,1] == 'fixed_defect') {
df['thal'][i,1] = as.numeric(6)
}
else if (df['thal'][i,1] == 'reversable_defect') {
df['thal'][i,1] = as.numeric(7)
}
else if (df['thal'][i,1] == 'NA') {
df['thal'][i,1] = as.numeric(0)
}
}
df <- within(df, rm(num, X))
df$age <- as.numeric(df$age)
df$sex <- as.numeric(df$sex)
df$cp <- as.numeric(df$cp)
df$trestbps <- as.numeric(df$trestbps)
df$chol <- as.numeric(df$chol)
df$fbs <- as.numeric(df$fbs)
df$restecg <- as.numeric(df$restecg)
df$thalach <- as.numeric(df$thalach)
df$exang <- as.numeric(df$exang)
df$oldpeak <- as.numeric(df$oldpeak)
df$slope <- as.numeric(df$slope)
df$ca <- as.numeric(df$ca)
df$thal <- as.numeric(df$thal)
df_norm <- as.data.frame(scale(df))
head(df_norm)
# K-Means
df_K <- kmeans(df_norm, centers = 2)
df_K
df_K3 <- kmeans(df_norm, centers = 3)
df_K3
df_K4 <- kmeans(df_norm, centers = 4)
df_K4
df_K5 <- kmeans(df_norm, centers = 5)
df_K5
df_K2
df_K
df_K$withinss
df_K
df_K3
df_K4
df_K5
df_K$tot.withinss
df_K3[1]
df_K3[2]
df_K3[3]
df_K3[4]
df_K3[5]
df_K3[6]
df_K3[7]
df_K3[8]
df_K3[9]
df_K3[10]
df_K
df_K3 <- kmeans(df_norm, centers = 3)
df_K3
df_K4 <- kmeans(df_norm, centers = 4)
df_K4
df_K5 <- kmeans(df_norm, centers = 5)
df_K5
## Hierarchical
dist_mat <- dist(df_norm, method = 'euclidean')
hclust_avg <- hclust(dist_mat, method = 'average')
plot(hclust_avg)
library(e1071) # all purpose machine learning package
options(scipen=999)
options(repr.plot.width = 15, repr.plot.height = 20)
library(caret)
library(pROC)
library(tidyverse)
library(yardstick)
library(tidyverse)
library(randomForest)
library(rpart)
library(ggplot2)
options(repr.plot.width = 15, repr.plot.height = 20)
df<- read.csv("2. Classification/heart_disease_modified.csv")
df <- within(df, rm(X, Patient_ID))
df <- na.omit(df)
df$class <- as.factor(df$class)
## Split data into training and testing
bound <- floor((nrow(df)/4)*3)   # 75-25% training testing split
df <- df[sample(nrow(df)),]          # shuffles the data
df_train <- df[1:bound, ]              # get training set
df_test <- df[(bound+1):nrow(df), ]    # get test set
### Naive Bayes
nb_model <- naiveBayes(class ~ ., data = df_train)
####### predicting and plotting CM
nb_pred <- predict(nb_model, newdata = df_test)
table(predicted = nb_pred, observed = df_test$class)
confusionMatrix(nb_pred, df_test$class)
library(randomForest)
library(ROCR)
forest_train <- randomForest(class ~ ., data = df_train)
print(forest_train) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train)
testforest = predict(forest_train, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
confusionMatrix(testforest, df_test$class)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 800, mtry = 4, maxnodes=24)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
confusionMatrix(nb_pred, df_test$class)
confusionMatrix(testforest, df_test$class)
confusionMatrix(testforest2, df_test$class)
varImpPlot(forest_train)
varImpPlot(nb_model)
##### AUC values
auc_nb <- performance(nbpred, measure = "auc")
auc_nb <- auc_nb@y.values[[1]]
print(auc_nb)
