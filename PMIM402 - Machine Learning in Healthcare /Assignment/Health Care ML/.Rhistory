}
}
print(df['sex'])
for (i in 1:nrow(df['cp'])){
if (df['cp'][i,1] == 'typ_angina') {
df['cp'][i,1] = as.numeric(1)
}
else if (df['cp'][i,1] == 'atyp_angina') {
df['cp'][i,1] = as.numeric(2)
}
else if (df['cp'][i,1] == 'non_anginal') {
df['cp'][i,1] = as.numeric(3)
}
else if (df['cp'][i,1] == 'asympt') {
df['cp'][i,1] = as.numeric(4)
}
}
print(df['cp'])
for (i in 1:nrow(df['fbs'])){
if (df['fbs'][i,1] == 't') {
df['fbs'][i,1] = as.numeric(1)
}
else if (df['fbs'][i,1] == 'f') {
df['fbs'][i,1] = as.numeric(0)
}
}
print(df['fbs'])
for (i in 1:nrow(df['restecg'])){
if (df['restecg'][i,1] == 'normal') {
df['restecg'][i,1] = as.numeric(0)
}
else if (df['restecg'][i,1] == 'st_t_wave_abnormality') {
df['restecg'][i,1] = as.numeric(1)
}
else if (df['restecg'][i,1] == 'left_vent_hyper') {
df['restecg'][i,1] = as.numeric(2)
}
}
print(df['restecg'])
for (i in 1:nrow(df['exang'])){
if (df['exang'][i,1] == 'yes') {
df['exang'][i,1] = as.numeric(1)
}
else if (df['exang'][i,1] == 'no') {
df['exang'][i,1] = as.numeric(0)
}
}
print(df$exang)
for (i in 1:nrow(df['slope'])){
if (df['slope'][i,1] == 'up') {
df['slope'][i,1] = as.numeric(1)
}
else if (df['slope'][i,1] == 'flat') {
df['slope'][i,1] = as.numeric(2)
}
else if (df['slope'][i,1] == 'down') {
df['slope'][i,1] = as.numeric(3)
}
}
print(df['slope'])
for (i in 1:nrow(df['thal'])){
if (df['thal'][i,1] == 'normal') {
df['thal'][i,1] = as.numeric(3)
}
else if (df['thal'][i,1] == 'fixed_defect') {
df['thal'][i,1] = as.numeric(6)
}
else if (df['thal'][i,1] == 'reversable_defect') {
df['thal'][i,1] = as.numeric(7)
}
else if (df['thal'][i,1] == 'NA') {
df['thal'][i,1] = as.numeric(0)
}
}
print(df$thal[1])
df <- within(df, rm(num, X))
df$age <- as.numeric(df$age)
df$sex <- as.numeric(df$sex)
df$cp <- as.numeric(df$cp)
df$trestbps <- as.numeric(df$trestbps)
df$chol <- as.numeric(df$chol)
df$fbs <- as.numeric(df$fbs)
df$restecg <- as.numeric(df$restecg)
df$thalach <- as.numeric(df$thalach)
df$exang <- as.numeric(df$exang)
df$oldpeak <- as.numeric(df$oldpeak)
df$slope <- as.numeric(df$slope)
df$ca <- as.numeric(df$ca)
df$thal <- as.numeric(df$thal)
df_norm <- as.data.frame(scale(df))
head(df_norm)
df_K <- kmeans(df_norm, centers = 2)
df_K
df_K <- kmeans(df_norm, centers = 3)
df_K
df_K <- kmeans(df_norm, centers = 4)
df_K
df_K <- kmeans(df_norm, centers = 5)
df_K
## Hierarchical
dist_mat <- dist(df_norm, method = 'euclidean')
hclust_avg <- hclust(dist_mat, method = 'average')
plot(hclust_avg)
#install.packages("e1071")
#install.packages('factoextra')
library(e1071) # all purpose machine learning package
library(reshape2) # library to reshape data
library(ggplot2)
options(scipen=999)
library(tidyverse) # data manipulation
library(cluster) # clustering algorithms
library(factoextra)
options(repr.plot.width = 15, repr.plot.height = 20)
########################### Load in dataset ####################################
df<- read.csv("1. Clustering/heart-c.csv")
df_og <- df
df <- na.omit(df)
for (i in 1:nrow(df['sex'])){
if (df['sex'][i,1] == 'male') {
df['sex'][i,1] = as.numeric(1)
}
else if (df['sex'][i,1] == 'female') {
df['sex'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['cp'])){
if (df['cp'][i,1] == 'typ_angina') {
df['cp'][i,1] = as.numeric(1)
}
else if (df['cp'][i,1] == 'atyp_angina') {
df['cp'][i,1] = as.numeric(2)
}
else if (df['cp'][i,1] == 'non_anginal') {
df['cp'][i,1] = as.numeric(3)
}
else if (df['cp'][i,1] == 'asympt') {
df['cp'][i,1] = as.numeric(4)
}
}
for (i in 1:nrow(df['fbs'])){
if (df['fbs'][i,1] == 't') {
df['fbs'][i,1] = as.numeric(1)
}
else if (df['fbs'][i,1] == 'f') {
df['fbs'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['restecg'])){
if (df['restecg'][i,1] == 'normal') {
df['restecg'][i,1] = as.numeric(0)
}
else if (df['restecg'][i,1] == 'st_t_wave_abnormality') {
df['restecg'][i,1] = as.numeric(1)
}
else if (df['restecg'][i,1] == 'left_vent_hyper') {
df['restecg'][i,1] = as.numeric(2)
}
}
for (i in 1:nrow(df['exang'])){
if (df['exang'][i,1] == 'yes') {
df['exang'][i,1] = as.numeric(1)
}
else if (df['exang'][i,1] == 'no') {
df['exang'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['slope'])){
if (df['slope'][i,1] == 'up') {
df['slope'][i,1] = as.numeric(1)
}
else if (df['slope'][i,1] == 'flat') {
df['slope'][i,1] = as.numeric(2)
}
else if (df['slope'][i,1] == 'down') {
df['slope'][i,1] = as.numeric(3)
}
}
for (i in 1:nrow(df['thal'])){
if (df['thal'][i,1] == 'normal') {
df['thal'][i,1] = as.numeric(3)
}
else if (df['thal'][i,1] == 'fixed_defect') {
df['thal'][i,1] = as.numeric(6)
}
else if (df['thal'][i,1] == 'reversable_defect') {
df['thal'][i,1] = as.numeric(7)
}
else if (df['thal'][i,1] == 'NA') {
df['thal'][i,1] = as.numeric(0)
}
}
df <- within(df, rm(num, X))
df$age <- as.numeric(df$age)
df$sex <- as.numeric(df$sex)
df$cp <- as.numeric(df$cp)
df$trestbps <- as.numeric(df$trestbps)
df$chol <- as.numeric(df$chol)
df$fbs <- as.numeric(df$fbs)
df$restecg <- as.numeric(df$restecg)
df$thalach <- as.numeric(df$thalach)
df$exang <- as.numeric(df$exang)
df$oldpeak <- as.numeric(df$oldpeak)
df$slope <- as.numeric(df$slope)
df$ca <- as.numeric(df$ca)
df$thal <- as.numeric(df$thal)
df_norm <- as.data.frame(scale(df))
head(df_norm)
# K-Means
df_K <- kmeans(df_norm, centers = 2)
df_K
## Hierarchical
dist_mat <- dist(df_norm, method = 'euclidean')
hclust_avg <- hclust(dist_mat, method = 'average')
plot(hclust_avg)
df<- read.csv("2. Classification/heart_disease_modified.csv")
View(df)
df <- na.omit(df)
df_labels <- df %>% select(class)
df <- within(df, rm(X, Patient_ID, class))
print(df_labels)
print(df)
bound <- floor((nrow(df)/4)*3)   # 75-25% training testing split
df <- df[sample(nrow(df)),]          # shuffles the data
df.train <- df[1:bound, ]              # get training set
df.test <- df[(bound+1):nrow(df), ]    # get test set
df <- df[sample(nrow(df)),]          # shuffles the data
df_train <- df[1:bound, ]              # get training set
df_test <- df[(bound+1):nrow(df), ]    # get test set
#nb_model <- naiveBayes(labels ~ ., data = df.train)
#
##Output apriori and conditional probabilities
#print(nb_model)
print(df_train)
rffit <- randomForest(sex ~ .,df_train)
library(randomForest)
install.packages("randomForest")
library(rpart)
library(randomForest)
rffit <- randomForest(sex ~ .,df_train)
y
plot(rffit)
varImpPlot(rffit)
prediction_for_table <- predict(rffit,df_test)
table(observed=df_test,predicted=prediction_for_table)
prediction_for_table <- predict(rffit,df_test[,-5])
table(observed=df_test[,5],predicted=prediction_for_table)
library(ROCR)
install.packages("ROCR")
View(df)
df <- na.omit(df)
df <- within(df, rm(X, Patient_ID))
bound <- floor((nrow(df)/4)*3)   # 75-25% training testing split
df <- df[sample(nrow(df)),]          # shuffles the data
df_train <- df[1:bound, ]              # get training set
df_test <- df[(bound+1):nrow(df), ]    # get test set
rffit <- randomForest(class ~ .,df_train)
rffit <- randomForest(class,df_train)
rffit <- randomForest(df_train$class,df_train)
df<- read.csv("2. Classification/heart_disease_modified.csv")
df <- na.omit(df)
df <- within(df, rm(X, Patient_ID))
bound <- floor((nrow(df)/4)*3)   # 75-25% training testing split
df <- df[sample(nrow(df)),]          # shuffles the data
df_train <- df[1:bound, ]              # get training set
df_test <- df[(bound+1):nrow(df), ]    # get test set
rffit <- randomForest(df_train$class,df_train)
rffit <- randomForest(class ~ .,df_train)
plot(rffit)
varImpPlot(rffit)
prediction_for_table <- predict(rffit,df_test[,-5])
table(observed=df_test[,5],predicted=prediction_for_table)
library(ROCR)
# Calculate the probability of new observations belonging to each class
# prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
prediction_for_roc_curve <- predict(rf_classifier,validation1[,-5],type="prob")
# Use pretty colours:
pretty_colours <- c("#F8766D","#00BA38","#619CFF")
# Calculate the probability of new observations belonging to each class
# prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
prediction_for_roc_curve <- predict(rf_classifier,df_test[,-5],type="prob")
# Calculate the probability of new observations belonging to each class
# prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
prediction_for_roc_curve <- predict(rffit,df_test[,-5],type="prob")
# Use pretty colours:
pretty_colours <- c("#F8766D","#00BA38","#619CFF")
# Specify the different classes
classes <- levels(df_test$class)
# For each class
for (i in 1:2)
{
# Define which observations belong to class[i]
true_values <- ifelse(df_test[,5]==classes[i],1,0)
# Assess the performance of classifier for class[i]
pred <- prediction(prediction_for_roc_curve[,i],true_values)
perf <- performance(pred, "tpr", "fpr")
if (i==1)
{
plot(perf,main="ROC Curve",col=pretty_colours[i])
}
else
{
plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE)
}
# Calculate the AUC and print it to screen
auc.perf <- performance(pred, measure = "auc")
print(auc.perf@y.values)
}
for (i in 1:2)
{
# Define which observations belong to class[i]
true_values <- ifelse(df_test[,5]==classes[i],1,0)
# Assess the performance of classifier for class[i]
pred <- prediction(prediction_for_roc_curve[,i],true_values)
perf <- performance(pred, "tpr", "fpr")
if (i==1)
{
plot(perf,main="ROC Curve",col=pretty_colours[i])
}
else
{
plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE)
}
# Calculate the AUC and print it to screen
auc.perf <- performance(pred, measure = "auc")
print(auc.perf@y.values)
}
# Calculate the probability of new observations belonging to each class
# prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
prediction_for_roc_curve <- predict(rffit,df_test[,-5],type="prob")
rffit <- randomForest(df_train,df_train$class)
plot(rffit)
varImpPlot(rffit)
nb_model <- naiveBayes(class ~ ., data = df_train)
#
##Output apriori and conditional probabilities
print(nb_model)
nb_pred<-predict(nb_model, df_test)
table(predicted = nb_pred,observed = df_test$class)
nb_pred %>%
roc_curve(truth=df_test$class , probClass) %>%
autoplot()
library(yardstick)
library(caret)
install.packages("caret")
install.packages("yardstick")
install.packages("yardstick")
roc_curve(truth=df_test$class , probClass) %>%
autoplot()
library(tidyverse)
library(randomForest)
library(rpart)
nb_pred %>%
roc_curve(truth=df_test$class , probClass) %>%
autoplot()
library(ggplot2)
options(repr.plot.width = 15, repr.plot.height = 20)
nb_pred %>%
roc_curve(truth=df_test$class , probClass) %>%
autoplot()
library(tidyverse)
nb_pred %>%
roc_curve(truth=df_test$class , probClass) %>%
autoplot()
library(caret)
library(tidyverse)
library(yardstick)
install.packages("yardstick")
install.packages("yardstick")
Yes
yes
install.packages("caret")
library(caret)
library(tidyverse)
library(yardstick)
install.packages("yardstick")
install.packages("yardstick")
library(caret)
library(tidyverse)
library(yardstick)
nb_pred %>%
roc_curve(truth=df_test$class , probClass) %>%
autoplot()
df<- read.csv("2. Classification/heart_disease_modified.csv")
df <- na.omit(df)
df <- within(df, rm(X, Patient_ID))
print(df_labels)
df$class <- as.factor(df$class)
bound <- floor((nrow(df)/4)*3)   # 75-25% training testing split
df <- df[sample(nrow(df)),]          # shuffles the data
df_train <- df[1:bound, ]              # get training set
df_test <- df[(bound+1):nrow(df), ]    # get test set
nb_model <- naiveBayes(class ~ ., data = df_train)
library(caret)
library(tidyverse)
library(yardstick)
nb_model <- naiveBayes(class ~ ., data = df_train)
install.packages("proc")
install.packages("pROC")
library(pROC)
roc(class, nb_pred.fit$fitted.values, plot=TRUE)
roc(class, nb_pred$fitted.values, plot=TRUE)
library(e1071) # all purpose machine learning package
options(scipen=999)
options(repr.plot.width = 15, repr.plot.height = 20)
library(caret)
library(pROC)
library(tidyverse)
library(yardstick)
library(tidyverse)
library(randomForest)
library(rpart)
library(ggplot2)
options(repr.plot.width = 15, repr.plot.height = 20)
df<- read.csv("2. Classification/heart_disease_modified.csv")
df <- na.omit(df)
df <- within(df, rm(X, Patient_ID))
df$class <- as.factor(df$class)
bound <- floor((nrow(df)/4)*3)   # 75-25% training testing split
df <- df[sample(nrow(df)),]          # shuffles the data
df_train <- df[1:bound, ]              # get training set
df_test <- df[(bound+1):nrow(df), ]    # get test set
### Naive Bayes
nb_model <- naiveBayes(class ~ ., data = df_train)
#
##Output apriori and conditional probabilities
print(nb_model)
table(predicted = nb_pred,observed = df_test$class)
roc(class, nb_pred$fitted.values, plot=TRUE)
roc(class, nb_pred$values, plot=TRUE)
nb_pred<-predict(nb_model, df_test)
table(predicted = nb_pred,observed = df_test$class)
roc(class, nb_pred$values, plot=TRUE)
roc(class, nb_pred$fitted.values, plot=TRUE)
roc(class, nb_pred, plot=TRUE)
print(nb_pred)
roc(df_test$class, nb_pred, plot=TRUE)
roc(nb_pred, df_test$class, plot=TRUE)
### Experimenting
library(ROCR)
prf <- performance(nb_pred, "tpr", "fpr")
pr <- prediction(nb_pred, df_test$class)
pr <- prediction(nb_pred, df_test$class)
pr <- prediction(nb_pred, df_test$class)
pr <- prediction(nb_model, df_test$class)
#### Random Forrest
rffit <- randomForest(df_train,df_train$class)
plot(rffit)
varImpPlot(rffit)
prediction_for_table <- predict(rffit,df_test[,-5])
prediction_for_table <- predict(rffit,df_test)
table(observed=df_test[,5],predicted=prediction_for_table)
#### Random Forrest
rffit <- randomForest(df_train$class, df_train)
plot(rffit)
varImpPlot(rffit)
prediction_for_table <- predict(rffit,df_test)
table(observed=df_test[,5],predicted=prediction_for_table)
df_train,
#### Random Forrest
rffit <- randomForest(df_train,df_train$class)
prediction_for_table <- predict(rffit,df_test)
table(observed=df_test[,5],predicted=prediction_for_table)
#### Random Forrest
rffit <- randomForest(df_train$class, df_train)
#### Random Forrest
rffit <- randomForest(df_train$class, df_train)
#### Random Forrest
rffit <- randomForest(df_train,df_train$class)
plot(rffit)
varImpPlot(rffit)
prediction_for_table <- predict(rffit,df_test[,-5])
prediction_for_table <- predict(rffit,df_test)
table(observed=df_test[,5],predicted=prediction_for_table)
table(observed=df_test,predicted=prediction_for_table)
prediction_for_table <- predict(rffit,df_test)
table(observed=df_test,predicted=prediction_for_table)
#### Random Forrest
rffit <- randomForest(df_train,df_train$class)
plot(rffit)
varImpPlot(rffit)
svmFit <- train(class ~., data = df_train,
method = "svmLinear",
trControl = control
)
print(nb_pred$results)
print(nb_pred)
ls(nb_model)
ls(nb_pred)
control <- trainControl(method="cv", number=5, savePredictions = TRUE, classProbs = TRUE)
control
svmFit <- train(class ~., data = df_train,
method = "svmLinear",
trControl = control
)
svmFit <- train(class ~ ., data = df_train,
method = "svmLinear",
trControl = control
)
head(df_train)
svmFit <- train(class ~ sex, age, pace_maker, cp, trestbps, chol, fbs, restecg, thalach, data = df_train,
method = "svmLinear",
trControl = control
)
svmFit <- train(class ~ sex + age + pace_maker + cp + trestbps + chol + fbs + restecg + thalach, data = df_train,
method = "svmLinear",
trControl = control
)
svmFit <- train(class ~ sex + age + pace_maker + cp + trestbps + chol + fbs + restecg + thalach, data = df_train,
trControl = control
)
control <- trainControl(method="cv", number=5, savePredictions = TRUE, classProbs = TRUE)
control
