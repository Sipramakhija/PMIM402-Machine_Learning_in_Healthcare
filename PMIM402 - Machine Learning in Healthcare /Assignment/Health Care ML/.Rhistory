}
else if (df['exang'][i,1] == 'no') {
df['exang'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['slope'])){
if (df['slope'][i,1] == 'up') {
df['slope'][i,1] = as.numeric(1)
}
else if (df['slope'][i,1] == 'flat') {
df['slope'][i,1] = as.numeric(2)
}
else if (df['slope'][i,1] == 'down') {
df['slope'][i,1] = as.numeric(3)
}
}
for (i in 1:nrow(df['thal'])){
if (df['thal'][i,1] == 'normal') {
df['thal'][i,1] = as.numeric(3)
}
else if (df['thal'][i,1] == 'fixed_defect') {
df['thal'][i,1] = as.numeric(6)
}
else if (df['thal'][i,1] == 'reversable_defect') {
df['thal'][i,1] = as.numeric(7)
}
else if (df['thal'][i,1] == 'NA') {
df['thal'][i,1] = as.numeric(0)
}
}
df <- within(df, rm(num, X))
df$age <- as.numeric(df$age)
df$sex <- as.numeric(df$sex)
df$cp <- as.numeric(df$cp)
df$trestbps <- as.numeric(df$trestbps)
df$chol <- as.numeric(df$chol)
df$fbs <- as.numeric(df$fbs)
df$restecg <- as.numeric(df$restecg)
df$thalach <- as.numeric(df$thalach)
df$exang <- as.numeric(df$exang)
df$oldpeak <- as.numeric(df$oldpeak)
df$slope <- as.numeric(df$slope)
df$ca <- as.numeric(df$ca)
df$thal <- as.numeric(df$thal)
df_norm <- as.data.frame(scale(df))
head(df_norm)
# K-Means
df_K <- kmeans(df_norm, centers = 2)
df_K
df_K3 <- kmeans(df_norm, centers = 3)
df_K3
df_K4 <- kmeans(df_norm, centers = 4)
df_K4
df_K5 <- kmeans(df_norm, centers = 5)
df_K5
df_K2
df_K
df_K$withinss
df_K
df_K3
df_K4
df_K5
df_K$tot.withinss
df_K3[1]
df_K3[2]
df_K3[3]
df_K3[4]
df_K3[5]
df_K3[6]
df_K3[7]
df_K3[8]
df_K3[9]
df_K3[10]
df_K
df_K3 <- kmeans(df_norm, centers = 3)
df_K3
df_K4 <- kmeans(df_norm, centers = 4)
df_K4
df_K5 <- kmeans(df_norm, centers = 5)
df_K5
## Hierarchical
dist_mat <- dist(df_norm, method = 'euclidean')
hclust_avg <- hclust(dist_mat, method = 'average')
plot(hclust_avg)
library(e1071) # all purpose machine learning package
options(scipen=999)
options(repr.plot.width = 15, repr.plot.height = 20)
library(caret)
library(pROC)
library(tidyverse)
library(yardstick)
library(tidyverse)
library(randomForest)
library(rpart)
library(ggplot2)
options(repr.plot.width = 15, repr.plot.height = 20)
df<- read.csv("2. Classification/heart_disease_modified.csv")
df <- within(df, rm(X, Patient_ID))
df <- na.omit(df)
df$class <- as.factor(df$class)
## Split data into training and testing
bound <- floor((nrow(df)/4)*3)   # 75-25% training testing split
df <- df[sample(nrow(df)),]          # shuffles the data
df_train <- df[1:bound, ]              # get training set
df_test <- df[(bound+1):nrow(df), ]    # get test set
### Naive Bayes
nb_model <- naiveBayes(class ~ ., data = df_train)
####### predicting and plotting CM
nb_pred <- predict(nb_model, newdata = df_test)
table(predicted = nb_pred, observed = df_test$class)
confusionMatrix(nb_pred, df_test$class)
library(randomForest)
library(ROCR)
forest_train <- randomForest(class ~ ., data = df_train)
print(forest_train) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train)
testforest = predict(forest_train, newdata=df_test)
table(testforest, df_test$class) #confusion matrix for test set
confusionMatrix(testforest, df_test$class)
### Optimizing A selected Model
forest_train2 <- randomForest(class ~ ., data = df_train, ntree = 800, mtry = 4, maxnodes=24)
print(forest_train2) #notice the number of trees, number of splits and the confusion matrix
plot(forest_train2)
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class) #confusion matrix for test set
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
confusionMatrix(nb_pred, df_test$class)
confusionMatrix(testforest, df_test$class)
confusionMatrix(testforest2, df_test$class)
varImpPlot(forest_train)
varImpPlot(nb_model)
##### AUC values
auc_nb <- performance(nbpred, measure = "auc")
auc_nb <- auc_nb@y.values[[1]]
print(auc_nb)
#install.packages("e1071")
#install.packages('factoextra')
library(e1071) # all purpose machine learning package
library(reshape2) # library to reshape data
library(ggplot2)
options(scipen=999)
library(tidyverse) # data manipulation
library(cluster) # clustering algorithms
library(factoextra)
options(repr.plot.width = 15, repr.plot.height = 20)
########################### Load in dataset ####################################
df<- read.csv("1. Clustering/heart-c.csv")
View(df)
df_og <- df
df <- na.omit(df)
for (i in 1:nrow(df['sex'])){
if (df['sex'][i,1] == 'male') {
df['sex'][i,1] = as.numeric(1)
}
else if (df['sex'][i,1] == 'female') {
df['sex'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['cp'])){
if (df['cp'][i,1] == 'typ_angina') {
df['cp'][i,1] = as.numeric(1)
}
else if (df['cp'][i,1] == 'atyp_angina') {
df['cp'][i,1] = as.numeric(2)
}
else if (df['cp'][i,1] == 'non_anginal') {
df['cp'][i,1] = as.numeric(3)
}
else if (df['cp'][i,1] == 'asympt') {
df['cp'][i,1] = as.numeric(4)
}
}
for (i in 1:nrow(df['fbs'])){
if (df['fbs'][i,1] == 't') {
df['fbs'][i,1] = as.numeric(1)
}
else if (df['fbs'][i,1] == 'f') {
df['fbs'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['restecg'])){
if (df['restecg'][i,1] == 'normal') {
df['restecg'][i,1] = as.numeric(0)
}
else if (df['restecg'][i,1] == 'st_t_wave_abnormality') {
df['restecg'][i,1] = as.numeric(1)
}
else if (df['restecg'][i,1] == 'left_vent_hyper') {
df['restecg'][i,1] = as.numeric(2)
}
}
for (i in 1:nrow(df['exang'])){
if (df['exang'][i,1] == 'yes') {
df['exang'][i,1] = as.numeric(1)
}
else if (df['exang'][i,1] == 'no') {
df['exang'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['slope'])){
if (df['slope'][i,1] == 'up') {
df['slope'][i,1] = as.numeric(1)
}
else if (df['slope'][i,1] == 'flat') {
df['slope'][i,1] = as.numeric(2)
}
else if (df['slope'][i,1] == 'down') {
df['slope'][i,1] = as.numeric(3)
}
}
for (i in 1:nrow(df['thal'])){
if (df['thal'][i,1] == 'normal') {
df['thal'][i,1] = as.numeric(3)
}
else if (df['thal'][i,1] == 'fixed_defect') {
df['thal'][i,1] = as.numeric(6)
}
else if (df['thal'][i,1] == 'reversable_defect') {
df['thal'][i,1] = as.numeric(7)
}
else if (df['thal'][i,1] == 'NA') {
df['thal'][i,1] = as.numeric(0)
}
}
df <- within(df, rm(num, X))
df$age <- as.numeric(df$age)
df$sex <- as.numeric(df$sex)
df$cp <- as.numeric(df$cp)
df$trestbps <- as.numeric(df$trestbps)
df$chol <- as.numeric(df$chol)
df$fbs <- as.numeric(df$fbs)
df$restecg <- as.numeric(df$restecg)
df$thalach <- as.numeric(df$thalach)
df$exang <- as.numeric(df$exang)
df$oldpeak <- as.numeric(df$oldpeak)
df$slope <- as.numeric(df$slope)
df$ca <- as.numeric(df$ca)
df$thal <- as.numeric(df$thal)
df_norm <- as.data.frame(scale(df))
# K-Means
df_K <- kmeans(df_norm, centers = 2)
df_K
fviz_cluster(df_K, geom = "point", data = df)
df_K3 <- kmeans(df_norm, centers = 3)
df_K3
fviz_cluster(df_K3, geom = "point", data = df)
df_K4 <- kmeans(df_norm, centers = 4)
df_K4
fviz_cluster(df_K4, geom = "point", data = df)
df_K5 <- kmeans(df_norm, centers = 5)
df_K5
fviz_cluster(df_K5, geom = "point", data = df)
library(e1071) # all purpose machine learning package
options(scipen=999)
options(repr.plot.width = 15, repr.plot.height = 20)
library(caret)
library(pROC)
library(tidyverse)
library(yardstick)
library(tidyverse)
library(randomForest)
library(rpart)
library(ggplot2)
options(repr.plot.width = 15, repr.plot.height = 20)
install.packages("randomForest")
#install.packages("randomForest")
library(randomForest)
library(ROCR)
########################### Load in dataset ####################################
df<- read.csv("2. Classification/heart_disease_modified.csv")
################# Removing non-required data features ##########################
df <- within(df, rm(X, Patient_ID))
df <- na.omit(df)
df$class <- as.factor(df$class)
#df_labels <- df %>% select(class)
print(df_labels)
print(df)
############### Split data into training and testin ############################
bound <- floor((nrow(df)/4)*3)   # 75-25% training testing split
df <- df[sample(nrow(df)),]          # shuffles the data
df_train <- df[1:bound, ]              # get training set
df_test <- df[(bound+1):nrow(df), ]    # get test set
######################## Naive Bayes Model #####################################
nb_model <- naiveBayes(class ~ ., data = df_train)
print(nb_model)
# predicting and plotting simple CM
nb_pred <- predict(nb_model, newdata = df_test)
table(predicted = nb_pred, observed = df_test$class)
# ROC
test.nb = predict(nb_model, type = "raw", newdata = df_test)
nbpred = prediction(test.nb[,2], df_test$class)
nbperf = performance(nbpred, "tpr", "fpr")
plot(nbperf, main="Naive Bayes ROC", colorize=T)#
plot(nbperf, col=1, add=TRUE)
legend(0.6, 0.6, c('Naive Bayes'), 1:3)
# More detailed CM
confusionMatrix(nb_pred, df_test$class)
##### AUC values
auc_nb <- performance(nbpred, measure = "auc")
auc_nb <- auc_nb@y.values[[1]]
print(auc_nb)
###################### Random Forrest ##########################################
forest_train <- randomForest(class ~ ., data = df_train)
print(forest_train)
plot(forest_train)
# Prediction and simple CM
testforest = predict(forest_train, newdata=df_test)
table(testforest, df_test$class)
# ROC
test.forest = predict(forest_train, type = "prob", newdata = df_test)
forestpred = prediction(test.forest[,2], df_test$class)
forestperf = performance(forestpred, "tpr", "fpr")
plot(forestperf, main="Random Forrest ROC", colorize=T)#
plot(forestperf, col=1, add=TRUE)
legend(0.6, 0.6, c('rforest'), 1:3)
# Feature Importance
varImpPlot(forest_train)
# More detailed CM
confusionMatrix(testforest, df_test$class)
# AUC
auc_rf <- performance(forestpred, measure = "auc")
auc_rf <- auc_rf@y.values[[1]]
print(auc_rf)
################ Optimizing A selected Model ###################################
forest_train2 <- randomForest(class ~ ., data = df_train,
ntree = 800, mtry = 4,
maxnodes=24)
print(forest_train2)
plot(forest_train2)
# Predict and simple CM
testforest2 = predict(forest_train2, newdata=df_test)
table(testforest2, df_test$class)
# ROC compare against 3 Models
test.forest2 = predict(forest_train2, type = "prob", newdata = df_test)
forestpred2 = prediction(test.forest2[,2], df_test$class)
forestperf2 = performance(forestpred2, "tpr", "fpr")
plot(forestperf2, main="ROC for RF, NB and optimised Models", colorize=T)#
plot(forestperf, col=1, add=TRUE)#
plot(nbperf, col=2, add=TRUE)#
plot(forestperf2, col=3, add=TRUE)
legend(0.6, 0.6, c('rforest', 'nb', 'rforest opt'), 1:3)
# AUC
auc_rf2 <- performance(forestpred2, measure = "auc")
auc_rf2 <- auc_rf2@y.values[[1]]
print(auc_rf2)
#################### Comparing Models ###########################################
# Confusion Matrix
confusionMatrix(nb_pred, df_test$class)
confusionMatrix(testforest, df_test$class)
confusionMatrix(testforest2, df_test$class)
# AUC
auc_nb <- performance(nbpred, measure = "auc")
auc_nb <- auc_nb@y.values[[1]]
auc_rf <- performance(forestpred, measure = "auc")
auc_rf <- auc_rf@y.values[[1]]
auc_rf2 <- performance(forestpred2, measure = "auc")
auc_rf2 <- auc_rf2@y.values[[1]]
print(auc_nb)
print(auc_rf)
print(auc_rf2)
#install.packages("e1071")
#install.packages('factoextra')
library(e1071) # all purpose machine learning package
library(reshape2) # library to reshape data
library(ggplot2)
options(scipen=999)
library(tidyverse) # data manipulation
library(cluster) # clustering algorithms
library(factoextra)
options(repr.plot.width = 15, repr.plot.height = 20)
########################### Load in dataset ####################################
df<- read.csv("1. Clustering/heart-c.csv")
View(df)
df <- na.omit(df)
################# Converting Data to Numerical Values ##########################
for (i in 1:nrow(df['sex'])){
if (df['sex'][i,1] == 'male') {
df['sex'][i,1] = as.numeric(1)
}
else if (df['sex'][i,1] == 'female') {
df['sex'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['cp'])){
if (df['cp'][i,1] == 'typ_angina') {
df['cp'][i,1] = as.numeric(1)
}
else if (df['cp'][i,1] == 'atyp_angina') {
df['cp'][i,1] = as.numeric(2)
}
else if (df['cp'][i,1] == 'non_anginal') {
df['cp'][i,1] = as.numeric(3)
}
else if (df['cp'][i,1] == 'asympt') {
df['cp'][i,1] = as.numeric(4)
}
}
for (i in 1:nrow(df['fbs'])){
if (df['fbs'][i,1] == 't') {
df['fbs'][i,1] = as.numeric(1)
}
else if (df['fbs'][i,1] == 'f') {
df['fbs'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['restecg'])){
if (df['restecg'][i,1] == 'normal') {
df['restecg'][i,1] = as.numeric(0)
}
else if (df['restecg'][i,1] == 'st_t_wave_abnormality') {
df['restecg'][i,1] = as.numeric(1)
}
else if (df['restecg'][i,1] == 'left_vent_hyper') {
df['restecg'][i,1] = as.numeric(2)
}
}
for (i in 1:nrow(df['exang'])){
if (df['exang'][i,1] == 'yes') {
df['exang'][i,1] = as.numeric(1)
}
else if (df['exang'][i,1] == 'no') {
df['exang'][i,1] = as.numeric(0)
}
}
for (i in 1:nrow(df['slope'])){
if (df['slope'][i,1] == 'up') {
df['slope'][i,1] = as.numeric(1)
}
else if (df['slope'][i,1] == 'flat') {
df['slope'][i,1] = as.numeric(2)
}
else if (df['slope'][i,1] == 'down') {
df['slope'][i,1] = as.numeric(3)
}
}
for (i in 1:nrow(df['thal'])){
if (df['thal'][i,1] == 'normal') {
df['thal'][i,1] = as.numeric(3)
}
else if (df['thal'][i,1] == 'fixed_defect') {
df['thal'][i,1] = as.numeric(6)
}
else if (df['thal'][i,1] == 'reversable_defect') {
df['thal'][i,1] = as.numeric(7)
}
else if (df['thal'][i,1] == 'NA') {
df['thal'][i,1] = as.numeric(0)
}
}
################# Removing non-required data features ##########################
df <- within(df, rm(num, X))
##################### Casting data to numeric type #############################
df$age <- as.numeric(df$age)
df$sex <- as.numeric(df$sex)
df$cp <- as.numeric(df$cp)
df$trestbps <- as.numeric(df$trestbps)
df$chol <- as.numeric(df$chol)
df$fbs <- as.numeric(df$fbs)
df$restecg <- as.numeric(df$restecg)
df$thalach <- as.numeric(df$thalach)
df$exang <- as.numeric(df$exang)
df$oldpeak <- as.numeric(df$oldpeak)
df$slope <- as.numeric(df$slope)
df$ca <- as.numeric(df$ca)
df$thal <- as.numeric(df$thal)
######################## Normalising the data ##################################
df_norm <- as.data.frame(scale(df))
head(df_norm)
######################### K-Means Clustering ###################################
df_K <- kmeans(df_norm, centers = 2)
df_K
fviz_cluster(df_K, geom = "point", data = df)
df_K3 <- kmeans(df_norm, centers = 3)
df_K3
fviz_cluster(df_K3, geom = "point", data = df)
df_K4 <- kmeans(df_norm, centers = 4)
df_K4
fviz_cluster(df_K4, geom = "point", data = df)
df_K5 <- kmeans(df_norm, centers = 5)
df_K5
fviz_cluster(df_K5, geom = "point", data = df)
df_K
df_K3
df_K4
df_K5
######################## Hierarchical Clustering ###############################
dist_mat <- dist(df_norm, method = 'euclidean')
hclust_avg <- hclust(dist_mat, method = 'average')
plot(hclust_avg)
print(df_K5$tot.withinss)
# ROC
test_nb = predict(nb_model, type = "raw", newdata = df_test)
nbpred = prediction(test_nb[,2], df_test$class)
nbperf = performance(nbpred, "tpr", "fpr")
plot(nbperf, main="Naive Bayes ROC", colorize=T)#
plot(nbperf, col=1, add=TRUE)
legend(0.6, 0.6, c('Naive Bayes'), 1:3)
View(df)
########################### Load in dataset ####################################
df<- read.csv("1. Clustering/heart-c.csv")
View(df)
as.numeric(df$thal)
print(df$thal)
as.character(df$thal) %>% as.numeric
as.character(df$thal) %>% as.numeric()
